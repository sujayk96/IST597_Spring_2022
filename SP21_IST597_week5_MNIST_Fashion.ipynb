{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SP21_IST597_week5_MNIST_Fashion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujayk96/IST597_Spring_2022/blob/main/SP21_IST597_week5_MNIST_Fashion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(248)\n",
        "tf.random.set_seed(248)\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV-3kEaggcO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fe1234-f727-43e9-f1a6-672ab42862c9"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Load MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "    x_train1, x_val, y_train1, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
        "    return (x_train1, y_train1), (x_val, y_val), (x_test, y_test)"
      ],
      "metadata": {
        "id": "QiT5f8b6CL4u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SJzVI207LGsY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "source": [
        "size_input = 784\n",
        "size_hidden = [128, 64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000\n",
        "number_of_test_examples = 10000"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm23CzRihaW0"
      },
      "source": [
        "(X_train, y_train),(X_val, y_val), (X_test, y_test) = load_data()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "xMUgnT2nO8TX",
        "outputId": "960cc9f7-613f-4450-82cf-25ba3544dd49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 784) (54000, 10)\n",
            "(6000, 784) (6000, 10)\n",
            "(10000, 784) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(100)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(50)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    # self.W0 = tf.Variable(tf.random.normal([self.size_input, self.size_input]))\n",
        "    # # Initialize biases for hidden layer\n",
        "    # self.b0 = tf.Variable(tf.random.normal([1, self.size_input]))\n",
        "    self.W1 = tf.Variable(tf.math.multiply(tf.random.normal([self.size_input, self.size_hidden[0]]), 0.05) )\n",
        "    # Initialize biases for hidden layer\n",
        "    #print(self.W1)\n",
        "    self.b1 = tf.Variable(tf.math.multiply(tf.random.normal([1, self.size_hidden[0]]), 0.05))\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W2 = tf.Variable(tf.math.multiply(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]), 0.05))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.math.multiply(tf.random.normal([1, self.size_hidden[1]]), 0.05))\n",
        "    self.W3 = tf.Variable(tf.math.multiply(tf.random.normal([self.size_hidden[1], self.size_output]), 0.05))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.math.multiply(tf.random.normal([1, self.size_output]), 0.05))\n",
        "    # Define variables to be updated during backpropagation\n",
        "    #self.variables = [self.W0, self.b0,self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def forward_with_dp(self, X, dp):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output_dropout(X, dp)\n",
        "    else:\n",
        "      self.y = self.compute_output_dropout(X, dp)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    los = tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)\n",
        "    # print(y_true.shape)\n",
        "    # print(y_pred.shape)\n",
        "    # print(los)\n",
        "    return los\n",
        "\n",
        "  def loss_with_reg(self, y_pred, y_true, lambd):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    m = y_true.shape[0]\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    los = tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)\n",
        "    # L2_regularization_cost = (tf.reduce_sum(tf.square(self.W0)) + \n",
        "    #                           tf.reduce_sum(tf.square(self.W1)) + \n",
        "    #                           tf.reduce_sum(tf.square(self.W2)) + \n",
        "    #                           tf.reduce_sum(tf.square(self.W3)))*(lambd/(2*m))\n",
        "    \n",
        "    L2_regularization_cost = (tf.reduce_sum(tf.square(self.W1)) + \n",
        "                              tf.reduce_sum(tf.square(self.W2)) + \n",
        "                              tf.reduce_sum(tf.square(self.W3)))*(lambd/(2*m))\n",
        "    \n",
        "    reg_loss = los + L2_regularization_cost\n",
        "    #print(L2_regularization_cost, los, reg_loss)\n",
        "    return reg_loss\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #print(self.W1.shape)\n",
        "    #t1 = self.W1\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      \n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    #t2 = self.W1\n",
        "    # print(\"***************************************************\")\n",
        "    # print(len(grads))\n",
        "    # print(sum(grads[0]), sum(grads[1]), sum(grads[2]), sum(grads[3]), sum(grads[4]), sum(grads[5]))\n",
        "    # print(\"********************************************************************\")\n",
        "    # print(tf.reduce_sum(self.W1))\n",
        "  \n",
        "  def backward_with_dropout(self, X_train, y_train,dp):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #print(self.W1)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward_with_dp(X_train,dp)\n",
        "      \n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    #print(self.W1)\n",
        "\n",
        "  def backward_with_reg(self, X_train, y_train, reg):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #print(y_train.shape)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      \n",
        "      current_loss = self.loss_with_reg(predicted, y_train, reg)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables)) \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    Z0 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    A0 = tf.nn.relu(Z0)\n",
        "    A0 = tf.nn.batch_normalization(A0, tf.reduce_mean(A0), tf.math.reduce_variance(A0),None, None, 1e-12)\n",
        "    Z1 = tf.matmul(A0, self.W2) + self.b2\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    A1 = tf.nn.batch_normalization(A1, tf.reduce_mean(A1), tf.math.reduce_variance(A1), None, None, 1e-12)\n",
        "\n",
        "    Z2 = tf.matmul(A1, self.W3) + self.b3\n",
        "\n",
        "    output = tf.nn.softmax(Z2)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def compute_output_dropout(self, X, dp):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = (X_tf - tf.math.reduce_mean(X_tf)) / tf.math.reduce_variance(X_tf)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    ##Z0 = tf.matmul(X_tf, self.W0) + self.b0\n",
        "    #A0 = tf.nn.relu(Z0)\n",
        "    Z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    A1 = tf.nn.dropout(A1, dp)\n",
        "    # D1 = tf.random.uniform([A1.shape[0],A1.shape[1]])\n",
        "    # D1 = D1 < dp\n",
        "    # A1 = tf.where(D1, A1*1.0, A1*0.0)\n",
        "    # Compute output\n",
        "    Z2 = tf.matmul(A1, self.W2) + self.b2\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    A2 = tf.nn.dropout(A2, dp)\n",
        "    # D2 = tf.random.uniform([A2.shape[0],A2.shape[1]])\n",
        "    # D2 = D2 < dp\n",
        "    # A2 = tf.where(D2, A2*1.0, A2*0.0)\n",
        "    Z3 = tf.matmul(A2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(Z3)\n",
        "    #print(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model reg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "seeds = [123,4324,543,5290,9922,3456,1111,9999,8567,9944]\n",
        "seeds1 = [5290]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMFAuH18Ve0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69432dc-3615-4c68-9244-114f9b3b1398"
      },
      "source": [
        "history_list = []\n",
        "history_test_acc_0 = []\n",
        "maxpos = lambda x : np.argmax(x)\n",
        "\n",
        "for seed in seeds: \n",
        "  history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_loss': []\n",
        "  }\n",
        "  print(\"**********************************************************************************************************************************\")\n",
        "  NUM_EPOCHS = 5\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  REG_C = 0\n",
        "  reg = 0.001\n",
        "  dp = 0.7\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "\n",
        "    for inputs, outputs in train_ds:\n",
        "      \n",
        "\n",
        "      if REG_C == 0:\n",
        "        # Loss and backward without reg\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        \n",
        "        mlp_on_gpu.backward(inputs, outputs)\n",
        "        \n",
        "      elif REG_C == 1:\n",
        "        # Loss and backward with Regularization\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss_with_reg(preds, outputs, reg)\n",
        "        mlp_on_gpu.backward_with_reg(inputs, outputs, dp)\n",
        "      else:\n",
        "        # Loss and backward with Backprop\n",
        "        preds = mlp_on_gpu.forward_with_dp(inputs, dp)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        mlp_on_gpu.backward_with_dropout(inputs, outputs, reg)\n",
        "\n",
        "    train_loss = np.sum(loss_total_gpu) / X_train.shape[0]  \n",
        "\n",
        "      ###### Train accuracy\n",
        "    preds = mlp_on_gpu.forward(X_train)\n",
        "    yTrueMax = np.array([maxpos(rec) for rec in y_train])\n",
        "    yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "    train_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "    #print(\"Train accuracy:\", train_acc)\n",
        "\n",
        "    ######## Val loss and accuracy\n",
        "    val_loss = 0\n",
        "    if REG_C == 0:\n",
        "      # Loss and backward without reg\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc)        \n",
        "    elif REG_C == 1:\n",
        "      # Loss and backward with Regularization\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss_with_reg(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "    else:\n",
        "      # Loss and backward with Backprop\n",
        "      val_preds = mlp_on_gpu.forward_with_dp(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    print('Number of Epoch = {} - Average train loss:= {} - Average val loss:= {}, Train Acc:= {}, Val acc:= {}'.format(epoch + 1, train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "     \n",
        "       \n",
        "      \n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f} , {}'.format(time_taken, seed))\n",
        "\n",
        "\n",
        "\n",
        "  ##### Test Accuracy\n",
        "\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_gpu.forward(X_test)\n",
        "  yTrueMax = np.array([maxpos(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "  test_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  print(\"Test Accuracy:\", test_acc)\n",
        "  history_test_acc_0.append(test_acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.005954911408600984 - Average val loss:= 0.47576647996902466, Train Acc:= 0.8265555555555556, Val acc:= 0.8278333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.004111594588668258 - Average val loss:= 0.40750032663345337, Train Acc:= 0.8551481481481481, Val acc:= 0.849\n",
            "Number of Epoch = 3 - Average train loss:= 0.003702448809588397 - Average val loss:= 0.37810012698173523, Train Acc:= 0.8710925925925926, Val acc:= 0.8641666666666666\n",
            "Number of Epoch = 4 - Average train loss:= 0.003391010425708912 - Average val loss:= 0.36361613869667053, Train Acc:= 0.8796481481481482, Val acc:= 0.8653333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.0032235033953631367 - Average val loss:= 0.34186360239982605, Train Acc:= 0.8897777777777778, Val acc:= 0.8748333333333334\n",
            "\n",
            "Total time taken (in seconds): 198.90 , 123\n",
            "Test Accuracy: 0.8644\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.006008389508282697 - Average val loss:= 0.48989880084991455, Train Acc:= 0.8221481481481482, Val acc:= 0.8258333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.004146274707935474 - Average val loss:= 0.3959101438522339, Train Acc:= 0.8664814814814815, Val acc:= 0.8586666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.0037148188838252317 - Average val loss:= 0.3759241998195648, Train Acc:= 0.8735, Val acc:= 0.8621666666666666\n",
            "Number of Epoch = 4 - Average train loss:= 0.003410824811017072 - Average val loss:= 0.37483158707618713, Train Acc:= 0.8763888888888889, Val acc:= 0.8653333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.0032301079078956886 - Average val loss:= 0.3571244180202484, Train Acc:= 0.8851111111111111, Val acc:= 0.872\n",
            "\n",
            "Total time taken (in seconds): 165.04 , 4324\n",
            "Test Accuracy: 0.8601\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.006006679393627025 - Average val loss:= 0.46963101625442505, Train Acc:= 0.8284259259259259, Val acc:= 0.8298333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.00413157371238426 - Average val loss:= 0.40067967772483826, Train Acc:= 0.8610925925925926, Val acc:= 0.8551666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.0036977228235315395 - Average val loss:= 0.3778412342071533, Train Acc:= 0.8724074074074074, Val acc:= 0.8631666666666666\n",
            "Number of Epoch = 4 - Average train loss:= 0.0034379840426974825 - Average val loss:= 0.36112216114997864, Train Acc:= 0.8794259259259259, Val acc:= 0.8695\n",
            "Number of Epoch = 5 - Average train loss:= 0.0032148494014033564 - Average val loss:= 0.35190239548683167, Train Acc:= 0.886425925925926, Val acc:= 0.8705\n",
            "\n",
            "Total time taken (in seconds): 165.55 , 543\n",
            "Test Accuracy: 0.8595\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.005953883983470775 - Average val loss:= 0.4939661920070648, Train Acc:= 0.8218148148148148, Val acc:= 0.8231666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.004140356558340567 - Average val loss:= 0.41991740465164185, Train Acc:= 0.8545, Val acc:= 0.8506666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.0037031560827184605 - Average val loss:= 0.43494051694869995, Train Acc:= 0.850925925925926, Val acc:= 0.8485\n",
            "Number of Epoch = 4 - Average train loss:= 0.0034154024477358216 - Average val loss:= 0.3795976936817169, Train Acc:= 0.8720555555555556, Val acc:= 0.8623333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.003213712904188368 - Average val loss:= 0.3655078113079071, Train Acc:= 0.8822222222222222, Val acc:= 0.8688333333333333\n",
            "\n",
            "Total time taken (in seconds): 164.47 , 5290\n",
            "Test Accuracy: 0.8547\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0059456679732711225 - Average val loss:= 0.4991750419139862, Train Acc:= 0.8194074074074074, Val acc:= 0.8186666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.004108024032027633 - Average val loss:= 0.5008984208106995, Train Acc:= 0.8303148148148148, Val acc:= 0.8295\n",
            "Number of Epoch = 3 - Average train loss:= 0.0036718981707537617 - Average val loss:= 0.39108511805534363, Train Acc:= 0.8674074074074074, Val acc:= 0.8568333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.0033872926500108508 - Average val loss:= 0.3693464696407318, Train Acc:= 0.8796296296296297, Val acc:= 0.869\n",
            "Number of Epoch = 5 - Average train loss:= 0.0031941570705837675 - Average val loss:= 0.37063702940940857, Train Acc:= 0.881462962962963, Val acc:= 0.8706666666666667\n",
            "\n",
            "Total time taken (in seconds): 163.02 , 9922\n",
            "Test Accuracy: 0.8548\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.006011617024739583 - Average val loss:= 0.48569563031196594, Train Acc:= 0.8235740740740741, Val acc:= 0.8265\n",
            "Number of Epoch = 2 - Average train loss:= 0.0041873013531720195 - Average val loss:= 0.42887067794799805, Train Acc:= 0.8486481481481482, Val acc:= 0.8493333333333334\n",
            "Number of Epoch = 3 - Average train loss:= 0.003733992682562934 - Average val loss:= 0.4030369520187378, Train Acc:= 0.8582962962962963, Val acc:= 0.8558333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.003440011766221788 - Average val loss:= 0.3767256736755371, Train Acc:= 0.8701481481481481, Val acc:= 0.8658333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.0032203826904296873 - Average val loss:= 0.3571658134460449, Train Acc:= 0.8808148148148148, Val acc:= 0.8725\n",
            "\n",
            "Total time taken (in seconds): 165.70 , 3456\n",
            "Test Accuracy: 0.854\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.005997602109555845 - Average val loss:= 0.49813443422317505, Train Acc:= 0.8190185185185185, Val acc:= 0.8233333333333334\n",
            "Number of Epoch = 2 - Average train loss:= 0.004109821460865162 - Average val loss:= 0.3969985246658325, Train Acc:= 0.865462962962963, Val acc:= 0.8558333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.0036880103217230903 - Average val loss:= 0.3661212623119354, Train Acc:= 0.8783148148148148, Val acc:= 0.8688333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.0034178172923900463 - Average val loss:= 0.3663032650947571, Train Acc:= 0.8799444444444444, Val acc:= 0.8681666666666666\n",
            "Number of Epoch = 5 - Average train loss:= 0.003194430598506221 - Average val loss:= 0.37037980556488037, Train Acc:= 0.8805, Val acc:= 0.8663333333333333\n",
            "\n",
            "Total time taken (in seconds): 164.00 , 1111\n",
            "Test Accuracy: 0.8525\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.005948486328125 - Average val loss:= 0.46965551376342773, Train Acc:= 0.8261481481481482, Val acc:= 0.8268333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.004149599145959925 - Average val loss:= 0.45088696479797363, Train Acc:= 0.8375, Val acc:= 0.8358333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.0036978672168872973 - Average val loss:= 0.4115634858608246, Train Acc:= 0.8575, Val acc:= 0.8515\n",
            "Number of Epoch = 4 - Average train loss:= 0.0034120554040979454 - Average val loss:= 0.3679374158382416, Train Acc:= 0.8774259259259259, Val acc:= 0.8701666666666666\n",
            "Number of Epoch = 5 - Average train loss:= 0.0032097153840241607 - Average val loss:= 0.3493429720401764, Train Acc:= 0.8867962962962963, Val acc:= 0.8751666666666666\n",
            "\n",
            "Total time taken (in seconds): 163.58 , 9999\n",
            "Test Accuracy: 0.8617\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.006068014639395255 - Average val loss:= 0.5111306309700012, Train Acc:= 0.8158888888888889, Val acc:= 0.8161666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.0041708080150462965 - Average val loss:= 0.42684268951416016, Train Acc:= 0.8533518518518518, Val acc:= 0.8478333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.0037255545722113714 - Average val loss:= 0.37998729944229126, Train Acc:= 0.8722777777777778, Val acc:= 0.8611666666666666\n",
            "Number of Epoch = 4 - Average train loss:= 0.0034553369592737267 - Average val loss:= 0.35757389664649963, Train Acc:= 0.8833888888888889, Val acc:= 0.8688333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.0032588741161205153 - Average val loss:= 0.362547367811203, Train Acc:= 0.8838518518518519, Val acc:= 0.8685\n",
            "\n",
            "Total time taken (in seconds): 166.22 , 8567\n",
            "Test Accuracy: 0.8573\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.005995955855758102 - Average val loss:= 0.5077439546585083, Train Acc:= 0.8167037037037037, Val acc:= 0.8198333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.004178064416956019 - Average val loss:= 0.4068589210510254, Train Acc:= 0.8599444444444444, Val acc:= 0.8536666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.0037234587492766206 - Average val loss:= 0.42050591111183167, Train Acc:= 0.8540555555555556, Val acc:= 0.8525\n",
            "Number of Epoch = 4 - Average train loss:= 0.003441590768319589 - Average val loss:= 0.36329448223114014, Train Acc:= 0.8789259259259259, Val acc:= 0.8718333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.0032326431274414063 - Average val loss:= 0.36174046993255615, Train Acc:= 0.8807777777777778, Val acc:= 0.8705\n",
            "\n",
            "Total time taken (in seconds): 163.66 , 9944\n",
            "Test Accuracy: 0.858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_test_acc_0 = [0.8644,0.8601,0.8595,0.8525,0.8547,0.8548,0.854,0.8525,0.8617,0.8573,0.858]"
      ],
      "metadata": {
        "id": "3fj3yZpdGhXj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_list_1 = []\n",
        "history_test_acc_1 = []\n",
        "maxpos = lambda x : np.argmax(x)\n",
        "\n",
        "for seed in seeds: \n",
        "  history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_loss': []\n",
        "  }\n",
        "  print(\"**********************************************************************************************************************************\")\n",
        "  NUM_EPOCHS = 5\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  REG_C = 1\n",
        "  reg = 0.001\n",
        "  dp = 0.7\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "\n",
        "    for inputs, outputs in train_ds:\n",
        "      \n",
        "\n",
        "      if REG_C == 0:\n",
        "        # Loss and backward without reg\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        \n",
        "        mlp_on_gpu.backward(inputs, outputs)\n",
        "        \n",
        "      elif REG_C == 1:\n",
        "        # Loss and backward with Regularization\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss_with_reg(preds, outputs, reg)\n",
        "        mlp_on_gpu.backward_with_reg(inputs, outputs, dp)\n",
        "      else:\n",
        "        # Loss and backward with Backprop\n",
        "        preds = mlp_on_gpu.forward_with_dp(inputs, dp)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        mlp_on_gpu.backward_with_dropout(inputs, outputs, reg)\n",
        "\n",
        "    train_loss = np.sum(loss_total_gpu) / X_train.shape[0]  \n",
        "\n",
        "      ###### Train accuracy\n",
        "    preds = mlp_on_gpu.forward(X_train)\n",
        "    yTrueMax = np.array([maxpos(rec) for rec in y_train])\n",
        "    yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "    train_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "    #print(\"Train accuracy:\", train_acc)\n",
        "\n",
        "    ######## Val loss and accuracy\n",
        "    val_loss = 0\n",
        "    if REG_C == 0:\n",
        "      # Loss and backward without reg\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc)        \n",
        "    elif REG_C == 1:\n",
        "      # Loss and backward with Regularization\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss_with_reg(val_preds, y_val, reg)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "    else:\n",
        "      # Loss and backward with Backprop\n",
        "      val_preds = mlp_on_gpu.forward_with_dp(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    print('Number of Epoch = {} - Average train loss:= {} - Average val loss:= {}, Train Acc:= {}, Val acc:= {}'.format(epoch + 1, train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "     \n",
        "       \n",
        "      \n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f} , {}'.format(time_taken, seed))\n",
        "\n",
        "\n",
        "\n",
        "  ##### Test Accuracy\n",
        "\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_gpu.forward(X_test)\n",
        "  yTrueMax = np.array([maxpos(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "  test_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  print(\"Test Accuracy:\", test_acc)\n",
        "  history_test_acc_1.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCbCKWoYOWfI",
        "outputId": "c00a44b6-39f9-4d56-edb6-cd5997b195b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.016973507351345486 - Average val loss:= 1.305967092514038, Train Acc:= 0.6583888888888889, Val acc:= 0.6631666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.01131911892361111 - Average val loss:= 1.0010974407196045, Train Acc:= 0.7288148148148148, Val acc:= 0.7331666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.00919563576027199 - Average val loss:= 0.8528927564620972, Train Acc:= 0.7565925925925926, Val acc:= 0.759\n",
            "Number of Epoch = 4 - Average train loss:= 0.008035752473054108 - Average val loss:= 0.7622554302215576, Train Acc:= 0.7715925925925926, Val acc:= 0.7718333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.0072836241545500575 - Average val loss:= 0.7004392147064209, Train Acc:= 0.7826666666666666, Val acc:= 0.7818333333333334\n",
            "\n",
            "Total time taken (in seconds): 201.29 , 123\n",
            "Test Accuracy: 0.7789\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.01657678335684317 - Average val loss:= 1.2868285179138184, Train Acc:= 0.6525185185185185, Val acc:= 0.6551666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.011263599537037037 - Average val loss:= 0.9960277080535889, Train Acc:= 0.7293518518518518, Val acc:= 0.7336666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.009140644779911748 - Average val loss:= 0.8435249924659729, Train Acc:= 0.7583518518518518, Val acc:= 0.7601666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.007955263491030092 - Average val loss:= 0.7530961632728577, Train Acc:= 0.7723333333333333, Val acc:= 0.7768333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.007216864126699942 - Average val loss:= 0.6935110688209534, Train Acc:= 0.7833888888888889, Val acc:= 0.7846666666666666\n",
            "\n",
            "Total time taken (in seconds): 167.65 , 4324\n",
            "Test Accuracy: 0.7706\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.017660089563440392 - Average val loss:= 1.387311339378357, Train Acc:= 0.631537037037037, Val acc:= 0.6368333333333334\n",
            "Number of Epoch = 2 - Average train loss:= 0.012022805673104745 - Average val loss:= 1.0611319541931152, Train Acc:= 0.716462962962963, Val acc:= 0.716\n",
            "Number of Epoch = 3 - Average train loss:= 0.009650562427662037 - Average val loss:= 0.8923768401145935, Train Acc:= 0.7502222222222222, Val acc:= 0.7455\n",
            "Number of Epoch = 4 - Average train loss:= 0.008338767722800925 - Average val loss:= 0.7911566495895386, Train Acc:= 0.7670370370370371, Val acc:= 0.7685\n",
            "Number of Epoch = 5 - Average train loss:= 0.007512880678530093 - Average val loss:= 0.7236302495002747, Train Acc:= 0.7793888888888889, Val acc:= 0.7786666666666666\n",
            "\n",
            "Total time taken (in seconds): 168.99 , 543\n",
            "Test Accuracy: 0.7731\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.015985371907552084 - Average val loss:= 1.237784504890442, Train Acc:= 0.6806666666666666, Val acc:= 0.6821666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.010830998173466435 - Average val loss:= 0.9736416935920715, Train Acc:= 0.7369814814814815, Val acc:= 0.7401666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.008960030449761285 - Average val loss:= 0.8390889167785645, Train Acc:= 0.7667222222222222, Val acc:= 0.7678333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.007894673665364582 - Average val loss:= 0.755323052406311, Train Acc:= 0.7814444444444445, Val acc:= 0.7833333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.007201067889178241 - Average val loss:= 0.697964608669281, Train Acc:= 0.7892407407407407, Val acc:= 0.79\n",
            "\n",
            "Total time taken (in seconds): 176.31 , 5290\n",
            "Test Accuracy: 0.7803\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.016203826904296875 - Average val loss:= 1.2500054836273193, Train Acc:= 0.6757777777777778, Val acc:= 0.6795\n",
            "Number of Epoch = 2 - Average train loss:= 0.011020293059172454 - Average val loss:= 0.9787217974662781, Train Acc:= 0.7267777777777777, Val acc:= 0.7315\n",
            "Number of Epoch = 3 - Average train loss:= 0.009063323974609376 - Average val loss:= 0.8385974764823914, Train Acc:= 0.7507222222222222, Val acc:= 0.7575\n",
            "Number of Epoch = 4 - Average train loss:= 0.007960492734555845 - Average val loss:= 0.7526928782463074, Train Acc:= 0.7649259259259259, Val acc:= 0.7726666666666666\n",
            "Number of Epoch = 5 - Average train loss:= 0.007250702469437211 - Average val loss:= 0.6943477988243103, Train Acc:= 0.7761111111111111, Val acc:= 0.7848333333333334\n",
            "\n",
            "Total time taken (in seconds): 179.12 , 9922\n",
            "Test Accuracy: 0.7681\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.016616170247395833 - Average val loss:= 1.2857356071472168, Train Acc:= 0.664, Val acc:= 0.6645\n",
            "Number of Epoch = 2 - Average train loss:= 0.011294236924913194 - Average val loss:= 1.0061057806015015, Train Acc:= 0.7178888888888889, Val acc:= 0.7168333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.009254911634657118 - Average val loss:= 0.8612685203552246, Train Acc:= 0.743462962962963, Val acc:= 0.7458333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.008101907800745082 - Average val loss:= 0.7727582454681396, Train Acc:= 0.7600370370370371, Val acc:= 0.762\n",
            "Number of Epoch = 5 - Average train loss:= 0.0073611139368127895 - Average val loss:= 0.7123501896858215, Train Acc:= 0.7725555555555556, Val acc:= 0.7728333333333334\n",
            "\n",
            "Total time taken (in seconds): 170.01 , 3456\n",
            "Test Accuracy: 0.7612\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.017389698169849536 - Average val loss:= 1.336438536643982, Train Acc:= 0.6660925925925926, Val acc:= 0.6751666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.011597535309968172 - Average val loss:= 1.0239616632461548, Train Acc:= 0.7257777777777777, Val acc:= 0.7323333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.009432270191333912 - Average val loss:= 0.8722801208496094, Train Acc:= 0.7510370370370371, Val acc:= 0.7575\n",
            "Number of Epoch = 4 - Average train loss:= 0.008251332035771122 - Average val loss:= 0.7801879048347473, Train Acc:= 0.7675555555555555, Val acc:= 0.77\n",
            "Number of Epoch = 5 - Average train loss:= 0.007487789577907986 - Average val loss:= 0.7175723910331726, Train Acc:= 0.7790555555555555, Val acc:= 0.7821666666666667\n",
            "\n",
            "Total time taken (in seconds): 170.60 , 1111\n",
            "Test Accuracy: 0.771\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.016915146439163774 - Average val loss:= 1.2993391752243042, Train Acc:= 0.6422037037037037, Val acc:= 0.655\n",
            "Number of Epoch = 2 - Average train loss:= 0.011447596797236689 - Average val loss:= 1.0203514099121094, Train Acc:= 0.7124259259259259, Val acc:= 0.7231666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.009439206158673321 - Average val loss:= 0.8740482330322266, Train Acc:= 0.7474814814814815, Val acc:= 0.7571666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.008269108525028936 - Average val loss:= 0.7796660661697388, Train Acc:= 0.7679444444444444, Val acc:= 0.7728333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.007479218094437211 - Average val loss:= 0.7136578559875488, Train Acc:= 0.7813888888888889, Val acc:= 0.7858333333333334\n",
            "\n",
            "Total time taken (in seconds): 172.61 , 9999\n",
            "Test Accuracy: 0.7696\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.016739526819299768 - Average val loss:= 1.3155763149261475, Train Acc:= 0.6455, Val acc:= 0.6521666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.011514223451967592 - Average val loss:= 1.0215083360671997, Train Acc:= 0.7227222222222223, Val acc:= 0.7251666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.009374621921115451 - Average val loss:= 0.8661072850227356, Train Acc:= 0.7498333333333334, Val acc:= 0.756\n",
            "Number of Epoch = 4 - Average train loss:= 0.008164967289677373 - Average val loss:= 0.7723265886306763, Train Acc:= 0.7657592592592593, Val acc:= 0.7681666666666667\n",
            "Number of Epoch = 5 - Average train loss:= 0.007395020661530671 - Average val loss:= 0.7095553278923035, Train Acc:= 0.7775, Val acc:= 0.7796666666666666\n",
            "\n",
            "Total time taken (in seconds): 178.82 , 8567\n",
            "Test Accuracy: 0.7671\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.016870287859881365 - Average val loss:= 1.2860654592514038, Train Acc:= 0.6769259259259259, Val acc:= 0.6863333333333334\n",
            "Number of Epoch = 2 - Average train loss:= 0.011230622468171296 - Average val loss:= 0.9988780617713928, Train Acc:= 0.7276851851851852, Val acc:= 0.7338333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.009200360333478009 - Average val loss:= 0.8536108732223511, Train Acc:= 0.7511666666666666, Val acc:= 0.7556666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.008057487205222801 - Average val loss:= 0.7634850740432739, Train Acc:= 0.7674074074074074, Val acc:= 0.7738333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.007311365198206019 - Average val loss:= 0.7014029622077942, Train Acc:= 0.7801851851851852, Val acc:= 0.7868333333333334\n",
            "\n",
            "Total time taken (in seconds): 178.86 , 9944\n",
            "Test Accuracy: 0.774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_test_acc_1 = [0.7789,0.7706,0.7731,0.7803,0.7681,0.7612,0.771,0.7696,0.7671,0.774]"
      ],
      "metadata": {
        "id": "UMamV8wqG_sY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_list_2 = []\n",
        "history_test_acc_2 = []\n",
        "maxpos = lambda x : np.argmax(x)\n",
        "\n",
        "for seed in seeds: \n",
        "  history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_loss': []\n",
        "  }\n",
        "  print(\"**********************************************************************************************************************************\")\n",
        "  NUM_EPOCHS = 5\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  REG_C = 2\n",
        "  reg = 0.01\n",
        "  dp = 0.7\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "\n",
        "    for inputs, outputs in train_ds:\n",
        "      \n",
        "\n",
        "      if REG_C == 0:\n",
        "        # Loss and backward without reg\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        \n",
        "        mlp_on_gpu.backward(inputs, outputs)\n",
        "        \n",
        "      elif REG_C == 1:\n",
        "        # Loss and backward with Regularization\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss_with_reg(preds, outputs, reg)\n",
        "        mlp_on_gpu.backward_with_reg(inputs, outputs, dp)\n",
        "      else:\n",
        "        # Loss and backward with Backprop\n",
        "        preds = mlp_on_gpu.forward_with_dp(inputs, dp)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        mlp_on_gpu.backward_with_dropout(inputs, outputs, reg)\n",
        "\n",
        "    train_loss = np.sum(loss_total_gpu) / X_train.shape[0]  \n",
        "\n",
        "      ###### Train accuracy\n",
        "    preds = mlp_on_gpu.forward(X_train)\n",
        "    yTrueMax = np.array([maxpos(rec) for rec in y_train])\n",
        "    yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "    train_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "    #print(\"Train accuracy:\", train_acc)\n",
        "\n",
        "    ######## Val loss and accuracy\n",
        "    val_loss = 0\n",
        "    if REG_C == 0:\n",
        "      # Loss and backward without reg\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc)        \n",
        "    elif REG_C == 1:\n",
        "      # Loss and backward with Regularization\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss_with_reg(val_preds, y_val, reg)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "    else:\n",
        "      # Loss and backward with Backprop\n",
        "      val_preds = mlp_on_gpu.forward_with_dp(X_val, dp)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    print('Number of Epoch = {} - Average train loss:= {} - Average val loss:= {}, Train Acc:= {}, Val acc:= {}'.format(epoch + 1, train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "     \n",
        "       \n",
        "      \n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f} , {}'.format(time_taken, seed))\n",
        "\n",
        "\n",
        "\n",
        "  ##### Test Accuracy\n",
        "\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_gpu.forward(X_test)\n",
        "  yTrueMax = np.array([maxpos(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "  test_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  print(\"Test Accuracy:\", test_acc)\n",
        "  history_test_acc_2.append(test_acc)"
      ],
      "metadata": {
        "id": "sqgizto5OWV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85000b30-5576-4d79-a19a-7dae740892a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023052015516493055 - Average val loss:= 2.2884621620178223, Train Acc:= 0.18242592592592594, Val acc:= 0.134\n",
            "Number of Epoch = 2 - Average train loss:= 0.02272915084273727 - Average val loss:= 2.2566754817962646, Train Acc:= 0.33411111111111114, Val acc:= 0.16\n",
            "Number of Epoch = 3 - Average train loss:= 0.0223331140588831 - Average val loss:= 2.2060086727142334, Train Acc:= 0.44766666666666666, Val acc:= 0.17416666666666666\n",
            "Number of Epoch = 4 - Average train loss:= 0.021855830439814816 - Average val loss:= 2.155648708343506, Train Acc:= 0.5017222222222222, Val acc:= 0.19133333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.021291422526041666 - Average val loss:= 2.1060802936553955, Train Acc:= 0.518074074074074, Val acc:= 0.21533333333333332\n",
            "\n",
            "Total time taken (in seconds): 132.88 , 123\n",
            "Test Accuracy: 0.5108\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02305098243995949 - Average val loss:= 2.289816379547119, Train Acc:= 0.3025555555555556, Val acc:= 0.11583333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.022770539460358796 - Average val loss:= 2.263108015060425, Train Acc:= 0.37096296296296294, Val acc:= 0.13333333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.022426106770833332 - Average val loss:= 2.222728729248047, Train Acc:= 0.4169259259259259, Val acc:= 0.15083333333333335\n",
            "Number of Epoch = 4 - Average train loss:= 0.022053080240885415 - Average val loss:= 2.175544500350952, Train Acc:= 0.4657222222222222, Val acc:= 0.16716666666666666\n",
            "Number of Epoch = 5 - Average train loss:= 0.021601738823784723 - Average val loss:= 2.154615640640259, Train Acc:= 0.5034259259259259, Val acc:= 0.188\n",
            "\n",
            "Total time taken (in seconds): 112.10 , 4324\n",
            "Test Accuracy: 0.4959\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02314023166232639 - Average val loss:= 2.2936208248138428, Train Acc:= 0.16066666666666668, Val acc:= 0.12066666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.022822356047453703 - Average val loss:= 2.267293691635132, Train Acc:= 0.2827962962962963, Val acc:= 0.13516666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.02248298927589699 - Average val loss:= 2.2280454635620117, Train Acc:= 0.41074074074074074, Val acc:= 0.16633333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.022012385615596066 - Average val loss:= 2.1743502616882324, Train Acc:= 0.4766851851851852, Val acc:= 0.18033333333333335\n",
            "Number of Epoch = 5 - Average train loss:= 0.02140473994502315 - Average val loss:= 2.116673469543457, Train Acc:= 0.49442592592592594, Val acc:= 0.20883333333333334\n",
            "\n",
            "Total time taken (in seconds): 133.57 , 543\n",
            "Test Accuracy: 0.4993\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023157947681568286 - Average val loss:= 2.2989892959594727, Train Acc:= 0.16814814814814816, Val acc:= 0.11933333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.022826784487123844 - Average val loss:= 2.2653143405914307, Train Acc:= 0.2740740740740741, Val acc:= 0.137\n",
            "Number of Epoch = 3 - Average train loss:= 0.02248780201099537 - Average val loss:= 2.2311508655548096, Train Acc:= 0.34724074074074074, Val acc:= 0.156\n",
            "Number of Epoch = 4 - Average train loss:= 0.022095551667390047 - Average val loss:= 2.183182954788208, Train Acc:= 0.45840740740740743, Val acc:= 0.17483333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.021576323332609955 - Average val loss:= 2.1445531845092773, Train Acc:= 0.5115, Val acc:= 0.19783333333333333\n",
            "\n",
            "Total time taken (in seconds): 109.51 , 5290\n",
            "Test Accuracy: 0.5093\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023091035065827548 - Average val loss:= 2.2922449111938477, Train Acc:= 0.3453888888888889, Val acc:= 0.124\n",
            "Number of Epoch = 2 - Average train loss:= 0.02280018898292824 - Average val loss:= 2.2681868076324463, Train Acc:= 0.46635185185185185, Val acc:= 0.13616666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.02244489429615162 - Average val loss:= 2.225761651992798, Train Acc:= 0.4985, Val acc:= 0.15066666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.0219051513671875 - Average val loss:= 2.1705312728881836, Train Acc:= 0.5213703703703704, Val acc:= 0.16916666666666666\n",
            "Number of Epoch = 5 - Average train loss:= 0.021484103732638888 - Average val loss:= 2.1342968940734863, Train Acc:= 0.5452037037037037, Val acc:= 0.18433333333333332\n",
            "\n",
            "Total time taken (in seconds): 111.44 , 9922\n",
            "Test Accuracy: 0.5369\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02311795383029514 - Average val loss:= 2.2940027713775635, Train Acc:= 0.1869259259259259, Val acc:= 0.124\n",
            "Number of Epoch = 2 - Average train loss:= 0.022785237630208333 - Average val loss:= 2.257739305496216, Train Acc:= 0.32801851851851854, Val acc:= 0.15266666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.022399983723958333 - Average val loss:= 2.2210917472839355, Train Acc:= 0.4105925925925926, Val acc:= 0.15916666666666668\n",
            "Number of Epoch = 4 - Average train loss:= 0.021845316569010415 - Average val loss:= 2.1451423168182373, Train Acc:= 0.4794259259259259, Val acc:= 0.19883333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.02120607277199074 - Average val loss:= 2.0976574420928955, Train Acc:= 0.5381481481481482, Val acc:= 0.2135\n",
            "\n",
            "Total time taken (in seconds): 109.13 , 3456\n",
            "Test Accuracy: 0.5354\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023060777452256945 - Average val loss:= 2.293954372406006, Train Acc:= 0.2084814814814815, Val acc:= 0.1145\n",
            "Number of Epoch = 2 - Average train loss:= 0.022792631926359955 - Average val loss:= 2.2586464881896973, Train Acc:= 0.24424074074074073, Val acc:= 0.1365\n",
            "Number of Epoch = 3 - Average train loss:= 0.022455803765190973 - Average val loss:= 2.2284562587738037, Train Acc:= 0.3587962962962963, Val acc:= 0.15566666666666668\n",
            "Number of Epoch = 4 - Average train loss:= 0.021959400318287036 - Average val loss:= 2.1765599250793457, Train Acc:= 0.42933333333333334, Val acc:= 0.1825\n",
            "Number of Epoch = 5 - Average train loss:= 0.02137969970703125 - Average val loss:= 2.1059844493865967, Train Acc:= 0.4791111111111111, Val acc:= 0.20866666666666667\n",
            "\n",
            "Total time taken (in seconds): 110.03 , 1111\n",
            "Test Accuracy: 0.4725\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023169236924913194 - Average val loss:= 2.2980244159698486, Train Acc:= 0.22257407407407406, Val acc:= 0.11966666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.02287389910662616 - Average val loss:= 2.271470069885254, Train Acc:= 0.34909259259259257, Val acc:= 0.1285\n",
            "Number of Epoch = 3 - Average train loss:= 0.022501860441984954 - Average val loss:= 2.2266695499420166, Train Acc:= 0.38451851851851854, Val acc:= 0.15566666666666668\n",
            "Number of Epoch = 4 - Average train loss:= 0.021964380334924767 - Average val loss:= 2.171096086502075, Train Acc:= 0.3975740740740741, Val acc:= 0.18283333333333332\n",
            "Number of Epoch = 5 - Average train loss:= 0.02124092384620949 - Average val loss:= 2.093254804611206, Train Acc:= 0.4093148148148148, Val acc:= 0.211\n",
            "\n",
            "Total time taken (in seconds): 111.67 , 9999\n",
            "Test Accuracy: 0.4063\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023101279929832175 - Average val loss:= 2.289649486541748, Train Acc:= 0.2602407407407407, Val acc:= 0.142\n",
            "Number of Epoch = 2 - Average train loss:= 0.022735966435185186 - Average val loss:= 2.2547507286071777, Train Acc:= 0.3498888888888889, Val acc:= 0.15733333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.022308421947337963 - Average val loss:= 2.21435809135437, Train Acc:= 0.41885185185185186, Val acc:= 0.17766666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.021794679994936342 - Average val loss:= 2.162067174911499, Train Acc:= 0.4502407407407407, Val acc:= 0.1885\n",
            "Number of Epoch = 5 - Average train loss:= 0.0212098230432581 - Average val loss:= 2.0968945026397705, Train Acc:= 0.49687037037037035, Val acc:= 0.21883333333333332\n",
            "\n",
            "Total time taken (in seconds): 111.95 , 8567\n",
            "Test Accuracy: 0.4958\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023250714337384258 - Average val loss:= 2.30690860748291, Train Acc:= 0.2427777777777778, Val acc:= 0.11416666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.022913707591869212 - Average val loss:= 2.2788302898406982, Train Acc:= 0.2791296296296296, Val acc:= 0.1335\n",
            "Number of Epoch = 3 - Average train loss:= 0.022563103569878472 - Average val loss:= 2.240785598754883, Train Acc:= 0.32564814814814813, Val acc:= 0.15733333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.02211721236617477 - Average val loss:= 2.185880422592163, Train Acc:= 0.39085185185185184, Val acc:= 0.1775\n",
            "Number of Epoch = 5 - Average train loss:= 0.021554667154947915 - Average val loss:= 2.1295135021209717, Train Acc:= 0.48818518518518517, Val acc:= 0.2\n",
            "\n",
            "Total time taken (in seconds): 121.96 , 9944\n",
            "Test Accuracy: 0.4918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history_test_acc_0)\n",
        "print(history_test_acc_1)\n",
        "print(history_test_acc_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI4eH3PWrJRa",
        "outputId": "068ca8c5-b5db-4cf7-eb0b-d2b64ee642d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8644, 0.8601, 0.8595, 0.8525, 0.8547, 0.8548, 0.854, 0.8525, 0.8617, 0.8573]\n",
            "[0.7789, 0.7706, 0.7731, 0.7803, 0.7681, 0.7612, 0.771, 0.7696, 0.7671, 0.774]\n",
            "[0.5108, 0.4959, 0.4993, 0.5093, 0.5369, 0.5354, 0.4725, 0.4063, 0.4958, 0.4918]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {\"Normal\":history_test_acc_0,\n",
        "       \"With reg\": history_test_acc_1,\n",
        "       \"With dp\": history_test_acc_2}"
      ],
      "metadata": {
        "id": "iylWPO8brQjF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(dic.values())\n",
        "ax.set_xticklabels(dic.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "n2Z5NXB5sFIN",
        "outputId": "1835f274-d509-4379-c5e4-449238fda7bb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, 'Normal'), Text(0, 0, 'With reg'), Text(0, 0, 'With dp')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in dic.items():\n",
        "  print(k, \":   Mean: \",np.mean(v), \"Variance: \", np.var(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCQRBHCgsRpz",
        "outputId": "b3068da9-69fa-4b51-da45-8143dfb845bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal :   Mean:  0.8571500000000001 Variance:  1.5220499999999882e-05\n",
            "With reg :   Mean:  0.77139 Variance:  2.8256900000000102e-05\n",
            "With dp :   Mean:  0.49540000000000006 Variance:  0.0012244420000000007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zwfr3Q1Bsurz"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}